{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5237-mests/5237-mests/blob/main/UNDP_SETH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List all built-in Seaborn datasets"
      ],
      "metadata": {
        "id": "ve129psyuUd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# List all built-in Seaborn datasets\n",
        "sns.get_dataset_names()"
      ],
      "metadata": {
        "id": "CVj1bK9P4J0W",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA Pipeline on Titanic Dataset\n",
        "##Install Libraries"
      ],
      "metadata": {
        "id": "pVd79vF84cMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y numpy\n",
        "#!pip install --upgrade numpy\n",
        "#!pip install --upgrade --force-reinstall missingno shap umap-learn dowhy pandas-profiling\n",
        "# import numpy as np\n",
        "# print(np.__version__)  # Check if NumPy is properly installed\n",
        "#!pip install missingno shap umap-learn dowhy pandas-profiling"
      ],
      "metadata": {
        "id": "3Bsb-XCswuXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "_oVsBkVh4opE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import core libraries for data handling and analysis\n",
        "import pandas as pd  # For handling datasets in tabular format\n",
        "import numpy as np  # For numerical computations\n",
        "\n",
        "# Import libraries for visualization\n",
        "import matplotlib.pyplot as plt  # Basic plotting library\n",
        "import seaborn as sns  # Advanced statistical visualizations\n",
        "import missingno as msno  # For visualizing missing values\n",
        "\n",
        "# Import preprocessing utilities\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # For scaling and encoding data\n",
        "from sklearn.impute import SimpleImputer  # For handling missing values\n",
        "\n",
        "# Import dimensionality reduction techniques\n",
        "from sklearn.decomposition import PCA  # Principal Component Analysis (PCA)\n",
        "\n",
        "# Import machine learning models and outlier detection\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier  # Outlier detection & feature importance\n",
        "\n",
        "# Import SHAP for explainability and feature importance analysis\n",
        "import shap\n",
        "\n",
        "# Import UMAP for non-linear dimensionality reduction\n",
        "import umap\n",
        "\n",
        "# Import DoWhy for causal inference analysis\n",
        "# from dowhy import CausalModel\n",
        "import warnings\n",
        "\n",
        "# Suppress all warnings, including FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Suppress all warnings (including SettingWithCopyWarning and FutureWarning)\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n"
      ],
      "metadata": {
        "id": "usnCDnMjuV5O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Load & Inspect Titanic Dataset\n",
        "### a)Load Dataset"
      ],
      "metadata": {
        "id": "u4tGFIYvueMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "df"
      ],
      "metadata": {
        "id": "nIMTOEa20JYt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Head"
      ],
      "metadata": {
        "id": "fN5HqqoZ5-gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "nBARLtl30VRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c)Tail"
      ],
      "metadata": {
        "id": "382nH_006DDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "XpMS8U850ZMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) Information"
      ],
      "metadata": {
        "id": "PbgrK0E26K5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "p2yKqPE_0QPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e) Columns"
      ],
      "metadata": {
        "id": "aukevgY_8JUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "A7E1EuI40Ji-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### f) Shape"
      ],
      "metadata": {
        "id": "utowknPh8Qat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "w5J0gkV40Jls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### g) Data types"
      ],
      "metadata": {
        "id": "sYOvoZue8Z8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "5yE8fBNk5aSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "id": "3uedg2tz5lP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###h) Verify if df is a DataFrame"
      ],
      "metadata": {
        "id": "rrDpQagH8msi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "isinstance(df, pd.DataFrame)"
      ],
      "metadata": {
        "id": "rmb6PEzm7guy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###i) Duplicates"
      ],
      "metadata": {
        "id": "RfHW7K5H99EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated()"
      ],
      "metadata": {
        "id": "04nqifmD9_ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "MFdzY0LF-FjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### j) Uniqueness"
      ],
      "metadata": {
        "id": "P_ibNt5JAObz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "yZSJqFzDAWwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###k) Binary"
      ],
      "metadata": {
        "id": "lPnK6fVfB7i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df.isin([0, 1]).all()).sum()"
      ],
      "metadata": {
        "id": "gEIQzp0TC3Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns[df.isin([0, 1]).all()])"
      ],
      "metadata": {
        "id": "tgXuYbJ_DEmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) DATA CLEANING\n",
        "\n",
        "###a) Drop Duplicates"
      ],
      "metadata": {
        "id": "Z1pGYe8VELbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates()\n",
        "df"
      ],
      "metadata": {
        "id": "pp_lVISfLbVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Missing values"
      ],
      "metadata": {
        "id": "CJi3Ctw8LiZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing = df.isnull().sum()\n",
        "missing"
      ],
      "metadata": {
        "id": "sUjJL2SbEpqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing 'age' values grouped by 'pclass'\n",
        "missing_by_class = df[df['age'].isnull()].groupby('pclass').size()\n",
        "\n",
        "# Show the percentage of missing 'age' by 'pclass'\n",
        "missing_percentage_by_class = (missing_by_class / df['age'].isnull().sum()) * 100\n",
        "print(missing_percentage_by_class)\n"
      ],
      "metadata": {
        "id": "GyyNdqIKOoUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create an exploded pie chart with annotations\n",
        "fig = px.pie(\n",
        "    names=missing_percentage_by_class.index,\n",
        "    values=missing_percentage_by_class,\n",
        "    title=\"Missing 'Age' Values by Pclass\",\n",
        "    hole=0.3  # Create a donut chart\n",
        ")\n",
        "\n",
        "# Explode the slices to highlight each class\n",
        "fig.update_traces(pull=[0.1, 0.1, 0.1], textinfo='percent+label')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "EMpunAxhUsgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation Methods and Their Appropriate Usage\n",
        "\n",
        "| **Imputation Method** | **Appropriate When** | **Explanation**                                                                                                                                         |\n",
        "|-----------------------|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Mean**              | - Data is **normally distributed**. <br> - Data has **few outliers**. <br> - Missing data is **small in proportion** (less than 10-20%). | The **mean** is useful for **symmetric data** with **few outliers**. It is sensitive to outliers, so it should not be used when data is skewed.         |\n",
        "| **Median**            | - Data is **skewed**. <br> - Data contains **outliers**. <br> - Data is **continuous** (e.g., `age`, `fare`). <br> - Missing data is **moderate to high** in proportion. | The **median** is robust and works well for **skewed data** or when data contains **outliers**. It does not get affected by extreme values.               |\n",
        "| **Mode**              | - Data is **categorical** (e.g., `sex`, `pclass`, `embarked`). <br> - Missing data is **small in proportion** (less than 10-20%). <br> - Missing data is **random**. | The **mode** is ideal for **categorical data**, as it imputes the most frequent category. It is also used when the missing data is random and small.       |\n",
        "\n",
        "### **When to Use Each Method**:\n",
        "| **Method**  | **Typical Use Case**                                                |\n",
        "|-------------|---------------------------------------------------------------------|\n",
        "| **Mean**    | - Imputing `age`, `income`, `height` (if normally distributed).     |\n",
        "| **Median**  | - Imputing `age`, `fare`, `height` (if skewed or with outliers).    |\n",
        "| **Mode**    | - Imputing `sex`, `embarked`, `class`, `who` (categorical columns). |\n"
      ],
      "metadata": {
        "id": "vnbSiSNlTRP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot histogram for 'age'\n",
        "sns.histplot(df['age'], kde=True)\n",
        "plt.title('Distribution of Age')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sNeIbsjmUmmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate skewness for 'age' (or any continuous column)\n",
        "skewness = df['age'].skew()\n",
        "print(f\"Skewness of 'age': {skewness}\")"
      ],
      "metadata": {
        "id": "e_snHIDIU1w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skewness Analysis for `age` Column\n",
        "\n",
        "A skewness of **0.3674** for the `age` column indicates that the data is **slightly right-skewed** (positive skew), but it is not highly skewed. This means that the distribution of `age` has a tail on the right side, but it's not extreme.\n",
        "\n",
        "#### Interpretation of Skewness:\n",
        "- **Skewness between 0 and 1** typically indicates a **moderate right skew**.\n",
        "- **Skewness of 0** means the data is **perfectly symmetric**.\n",
        "- **Skewness > 1** indicates a more pronounced **right-skewed distribution**.\n",
        "- **Skewness < -1** indicates a **left-skewed distribution**.\n",
        "\n",
        "Given that the skewness of `age` is **0.3674**, it is a **mild positive skew**, meaning the data has more younger passengers with a few older ones (a longer tail on the right).\n",
        "\n",
        "### Action for Missing Data Imputation:\n",
        "Since the skewness is mild, it’s generally better to impute missing values with the **median** (as it is robust to skewed distributions) rather than the **mean**, which could be influenced by extreme age values.\n",
        "\n",
        "#### Suggested Approach for Imputation:\n",
        "Impute missing `age` values with the **median** of `age` grouped by **pclass** (passenger class), as **age** can vary by class. This will ensure that imputation considers the differences between passenger classes (e.g., first-class passengers may have a different age distribution than third-class passengers).\n"
      ],
      "metadata": {
        "id": "JuJ5Q1nPV-dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute 'age' with the median value by 'pclass'\n",
        "df['age'] = df.groupby('pclass')['age'].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# Check if there are any remaining missing values\n",
        "print(df['age'].isnull().sum())\n"
      ],
      "metadata": {
        "id": "siORugSaQ68a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For the `embarked` Variable:\n",
        "\n",
        "- **Missing Data**: The `embarked` column has a **0.2%** missing value.\n",
        "- **Imputation Strategy**: Given the low percentage, the missing values are likely **random**, so we will impute them with the **mode** (most frequent value).\n",
        "- **Action**: Use the **mode** to fill missing values in the `embarked` column.\n"
      ],
      "metadata": {
        "id": "SCrsKSDNW38z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
        "\n",
        "print(df['embarked'].isnull().sum())"
      ],
      "metadata": {
        "id": "mUTQPOcFW5Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For the `embark_town` Variable:\n",
        "\n",
        "- **Missing Data**: The `embark_town` column has **0.2%** missing values.\n",
        "- **Imputation Strategy**: Since the missing data is low, it’s likely random, so we can impute the missing values using the **mode** (most frequent value), as this column likely has a few categories.\n",
        "- **Action**: Impute the missing values in the `embark_town` column with the **mode**.\n"
      ],
      "metadata": {
        "id": "cinHaq7UY5Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute 'embark_town' with the mode\n",
        "df['embark_town'].fillna(df['embark_town'].mode()[0], inplace=True)\n",
        "\n",
        "# Check if there are any remaining missing values\n",
        "print(df['embark_town'].isnull().sum())"
      ],
      "metadata": {
        "id": "ZqPCgldFY5v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For the `deck` Variable:\n",
        "\n",
        "- **Missing Data**: The `deck` column has **77.2%** missing values.\n",
        "- **Imputation Strategy**: Given the high percentage, imputing is not ideal. We will **drop** the column instead.\n",
        "- **Action**: Drop the `deck` column from the dataset.\n"
      ],
      "metadata": {
        "id": "uKnIbf-bXLut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('deck', axis=1, inplace=True)\n",
        "df\n"
      ],
      "metadata": {
        "id": "5PZXMKDnXxHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "rs-YXQVjO28t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3S6BN0hBEmER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Categorical Vairiables\n",
        "#####a) Inconsistencies"
      ],
      "metadata": {
        "id": "cHytrsNxaQDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['sex'].unique())"
      ],
      "metadata": {
        "id": "-b3YrNPIaXSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize 'sex' values (convert to lowercase and strip any whitespace)\n",
        "df['sex'] = df['sex'].str.lower().str.strip()\n",
        "df['sex']"
      ],
      "metadata": {
        "id": "Tk6H_tLLaxj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for inconsistent values in 'embarked' column\n",
        "print(df['embarked'].unique())"
      ],
      "metadata": {
        "id": "IcaWdYG9a5ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize 'embarked' values (convert to uppercase and strip any whitespace)\n",
        "df['embarked'] = df['embarked'].str.upper().str.strip()\n",
        "df['embarked']"
      ],
      "metadata": {
        "id": "3AZ05hP5bFds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure valid categories for 'embarked' (C, Q, S)\n",
        "valid_embarked = ['C', 'Q', 'S']\n",
        "df = df[df['embarked'].isin(valid_embarked)]\n",
        "df"
      ],
      "metadata": {
        "id": "5cyPdJgobLRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values for 'who' column\n",
        "print(df['who'].unique())\n"
      ],
      "metadata": {
        "id": "PQANmeN_bjC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check unique values for 'embark_town' column\n",
        "print(df['embark_town'].unique())\n"
      ],
      "metadata": {
        "id": "fYLxjkVHbln6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values for 'alive' column (Yes/No)\n",
        "print(df['alive'].unique())\n"
      ],
      "metadata": {
        "id": "mmTDkGzEbnxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize 'alive' values to lowercase and strip spaces\n",
        "df['alive'] = df['alive'].str.lower().str.strip()\n",
        "df['alive']"
      ],
      "metadata": {
        "id": "nqOY5_nLbqxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'fare' contains any negative values\n",
        "print(df[df['fare'] < 0])"
      ],
      "metadata": {
        "id": "0DhSE8YPb6GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle negative 'fare' values (e.g., set them to NaN)\n",
        "df.loc[df['fare'] < 0, 'fare'] = None\n"
      ],
      "metadata": {
        "id": "aiWtcKADb-js"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'age' contains any unrealistic values (greater than 100 or negative)\n",
        "print(df[(df['age'] < 0) | (df['age'] > 100)])\n"
      ],
      "metadata": {
        "id": "SMV_hcJQb-sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle unrealistic 'age' values (e.g., set them to NaN)\n",
        "df.loc[(df['age'] < 0) | (df['age'] > 100), 'age'] = None\n"
      ],
      "metadata": {
        "id": "q1V7OTV6cGDT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding\n",
        "\n",
        "####a) Label Encoding"
      ],
      "metadata": {
        "id": "TqBJns66dcpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Label encode 'pclass' and 'sex'\n",
        "df['pclass'] = label_encoder.fit_transform(df['pclass'])\n",
        "df['sex'] = label_encoder.fit_transform(df['sex'])\n",
        "\n",
        "# Check the encoding\n",
        "print(df[['pclass', 'sex']].head(20))\n"
      ],
      "metadata": {
        "id": "Pos7ZcTxdi_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###b)Mapping"
      ],
      "metadata": {
        "id": "7xElM8X9d3tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping the 'embarked' column (C, Q, S)\n",
        "df['embarked'] = df['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "df['embarked']"
      ],
      "metadata": {
        "id": "aMYqak_xd9Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping the 'embark_town' column (Southampton, Cherbourg, Queenstown)\n",
        "df['embark_town'] = df['embark_town'].map({'Southampton': 0, 'Cherbourg': 1, 'Queenstown': 2})\n",
        "df['embark_town']"
      ],
      "metadata": {
        "id": "0BEUrKN6fiVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "13f_XwR8ch3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values in the 'class' column\n",
        "unique_class = df['class'].unique()\n",
        "print(\"Unique values in 'class' column:\", unique_class)\n"
      ],
      "metadata": {
        "id": "vtmFRmudgGQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map 'class' values to numerical values\n",
        "class_mapping = {'First': 1, 'Second': 2, 'Third': 3}\n",
        "df['class'] = df['class'].map(class_mapping)\n",
        "\n",
        "# Check the updated 'class' column\n",
        "print(df['class'].head())\n"
      ],
      "metadata": {
        "id": "5CLI_RWEch-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map 'who' values to numerical values\n",
        "who_mapping = {'man': 1, 'woman': 2, 'child': 3}\n",
        "df['who'] = df['who'].map(who_mapping)\n",
        "\n",
        "# Check the updated 'who' column\n",
        "print(df['who'].head())\n"
      ],
      "metadata": {
        "id": "ebAktkvpgTvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map 'adult_male' values to numerical values (True -> 1, False -> 0)\n",
        "df['adult_male'] = df['adult_male'].map({True: 1, False: 0})\n",
        "\n",
        "# Check the updated 'adult_male' column\n",
        "print(df['adult_male'].head())\n"
      ],
      "metadata": {
        "id": "_46CCL9mghm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map 'alive' values to numerical values ('yes' -> 1, 'no' -> 0)\n",
        "df['alive'] = df['alive'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Check the updated 'alive' column\n",
        "print(df['alive'].head())\n"
      ],
      "metadata": {
        "id": "Ha0m9BTSgxsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map 'alone' values to numerical values (True -> 1, False -> 0)\n",
        "df['alone'] = df['alone'].map({True: 1, False: 0})\n",
        "\n",
        "# Check the updated 'alone' column\n",
        "print(df['alone'].head())\n"
      ],
      "metadata": {
        "id": "b7OT7qSwhPMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any rows with text values (non-numeric) in the DataFrame\n",
        "text_rows = df.applymap(lambda x: isinstance(x, str))\n",
        "\n",
        "# Find rows with text values\n",
        "rows_with_text = df[text_rows.any(axis=1)]\n",
        "\n",
        "# Print rows with text values\n",
        "print(\"Rows with text values:\")\n",
        "print(rows_with_text)\n"
      ],
      "metadata": {
        "id": "HbzY-MSihzfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any rows with text values (non-numeric) in the DataFrame\n",
        "text_rows = df.applymap(lambda x: isinstance(x, str))\n",
        "\n",
        "# Find rows with text values\n",
        "rows_with_text = df[text_rows.any(axis=1)]\n",
        "\n",
        "# Print rows with text values\n",
        "print(\"Rows with text values:\")\n",
        "print(rows_with_text)\n"
      ],
      "metadata": {
        "id": "tzP-P3PIh5rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "tJQ_EtuqiMmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OUTLIERS\n",
        "\n",
        "### Z-Score:\n",
        "\n",
        "- Works best when the data is normally distributed.\n",
        "- Measures how far a data point is from the mean in terms of standard deviations.\n",
        "- It's sensitive to extreme outliers and is less effective if the data has a skewed distribution.\n",
        "\n",
        "### IQR:\n",
        "\n",
        "- More robust, as it focuses on the spread of the middle 50% of the data (between the 25th and 75th percentiles).\n",
        "- It is less influenced by extreme values or skewed data.\n",
        "- It works well even when the data is not normally distributed.\n",
        "\n",
        "### When to Use Each:\n",
        "\n",
        "- **Z-score**: Best used for **normally distributed** data or when you have a reasonable expectation that the data should follow a normal distribution.\n",
        "- **IQR**: Works better when the data is **skewed** or has a **non-normal distribution**. It's generally a safer choice in many cases.\n"
      ],
      "metadata": {
        "id": "KAbh8SfMmQCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Get all numerical columns\n",
        "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Adjust grid size based on number of numerical columns\n",
        "num_columns = len(numerical_columns)\n",
        "rows = (num_columns // 3) + (num_columns % 3 > 0)  # Determine number of rows needed\n",
        "cols = 3  # Keep 3 columns for the subplots\n",
        "\n",
        "# 1. Boxplot to visualize outliers for all numerical columns\n",
        "plt.figure(figsize=(15, 5 * rows))\n",
        "for i, column in enumerate(numerical_columns, 1):\n",
        "    plt.subplot(rows, cols, i)\n",
        "    sns.boxplot(df[column])\n",
        "    plt.title(f'Boxplot of {column}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bUkznmb7ip4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Z-score method to detect outliers\n",
        "z_scores = np.abs(stats.zscore(df[numerical_columns]))\n",
        "outliers_z = (z_scores > 3).all(axis=1)  # Outliers where z-score > 3\n",
        "print(f\"Number of outliers detected using Z-score: {np.sum(outliers_z)}\")"
      ],
      "metadata": {
        "id": "eZ0HaPeckR-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  IQR Method\n",
        "\n",
        "1. **Calculate the Quartiles**:\n",
        "\n",
        "   - **First Quartile ($Q1$)**: The 25th percentile of the data, which means that 25% of the data points lie below $Q1$.\n",
        "   \n",
        "   $$\n",
        "   Q1 = \\text{25th percentile of the data}\n",
        "   $$\n",
        "\n",
        "   - **Third Quartile ($Q3$)**: The 75th percentile of the data, meaning 75% of the data points lie below $Q3$.\n",
        "   \n",
        "   $$\n",
        "   Q3 = \\text{75th percentile of the data}\n",
        "   $$\n",
        "\n",
        "   - **Interquartile Range ($IQR$)**: The difference between the third and first quartile. It represents the middle 50% of the data.\n",
        "   \n",
        "   $$\n",
        "   IQR = Q3 - Q1\n",
        "   $$\n",
        "\n",
        "2. **Define the Outlier Thresholds**:\n",
        "\n",
        "   - **Lower Bound**: Any data point below this threshold is considered an outlier.\n",
        "   \n",
        "   $$\n",
        "   \\text{Lower Bound} = Q1 - 1.5 \\times IQR\n",
        "   $$\n",
        "\n",
        "   - **Upper Bound**: Any data point above this threshold is considered an outlier.\n",
        "   \n",
        "   $$\n",
        "   \\text{Upper Bound} = Q3 + 1.5 \\times IQR\n",
        "   $$\n",
        "\n",
        "3. **Outlier Detection**:\n",
        "\n",
        "   A data point $x_i$ is considered an outlier if it falls outside the range defined by the lower and upper bounds:\n",
        "\n",
        "   $$\n",
        "   x_i < Q1 - 1.5 \\times IQR \\quad \\text{or} \\quad x_i > Q3 + 1.5 \\times IQR\n",
        "   $$\n",
        "\n",
        "### Summary of Formulae:\n",
        "\n",
        "- **$Q1$** = 25th percentile\n",
        "- **$Q3$** = 75th percentile\n",
        "- **$IQR$** = $Q3 - Q1$\n",
        "- **Lower Bound** = $Q1 - 1.5 \\times IQR$\n",
        "- **Upper Bound** = $Q3 + 1.5 \\times IQR$\n",
        "\n",
        "### Outlier Condition:\n",
        "\n",
        "- $x_i$ is an outlier if:\n",
        "\n",
        "$$\n",
        "x_i < Q1 - 1.5 \\times IQR \\quad \\text{or} \\quad x_i > Q3 + 1.5 \\times IQR\n",
        "$$\n"
      ],
      "metadata": {
        "id": "HyOmJx-4nXgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Detect outliers using IQR method\n",
        "Q1 = df[numerical_columns].quantile(0.25)  # First quartile (25th percentile)\n",
        "Q3 = df[numerical_columns].quantile(0.75)  # Third quartile (75th percentile)\n",
        "IQR = Q3 - Q1  # Interquartile range\n",
        "\n",
        "# Identify outliers (values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR)\n",
        "outliers_iqr = ((df[numerical_columns] < (Q1 - 1.5 * IQR)) | (df[numerical_columns] > (Q3 + 1.5 * IQR)))\n",
        "\n",
        "# Count of rows with outliers in any of the numerical columns\n",
        "outliers_iqr_count = outliers_iqr.any(axis=1).sum()\n",
        "\n",
        "print(f\"Number of outliers detected using IQR: {outliers_iqr_count}\")\n"
      ],
      "metadata": {
        "id": "tTaLqKd6mnGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**  | **Outliers Detected**          | **Action**                           | **Reason**                                                                 |\n",
        "|---------------|---------------------------------|--------------------------------------|---------------------------------------------------------------------------|\n",
        "| **Age**       | Extreme values (e.g., >100)     | Cap to 99 or remove                  | Uncommon and may be erroneous; capping prevents distortion in model.       |\n",
        "| **Parch**     | High values (e.g., >6)          | Cap or remove                        | Rare, likely data entry errors; capping keeps model from being skewed.    |\n",
        "| **SibSp**     | High values (e.g., >6 or 7)     | Cap or remove                        | Rare occurrences; extreme values may be data errors.                       |\n",
        "| **Fare**      | High values (e.g., >500)        | Cap (95th percentile) or log transform| VIP/first-class passengers; capping/log reduces extreme influence on model. |\n"
      ],
      "metadata": {
        "id": "NVIEPR33lx_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max and min values for each variable of interest\n",
        "max_min_values = {\n",
        "    'Variable': ['Age', 'Parch', 'SibSp', 'Fare'],\n",
        "    'Max Value': [df['age'].max(), df['parch'].max(), df['sibsp'].max(), df['fare'].max()],\n",
        "    'Min Value': [df['age'].min(), df['parch'].min(), df['sibsp'].min(), df['fare'].min()]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "max_min_df = pd.DataFrame(max_min_values)\n",
        "\n",
        "# Show the table\n",
        "max_min_df\n"
      ],
      "metadata": {
        "id": "2dI_DlPwqWd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comments:\n",
        "- **Age**: The maximum value is 80, and the minimum value is 0.42. This range is reasonable since it covers a variety of ages from infants to elderly passengers.\n",
        "- **Parch**: The maximum value is 6, and the minimum value is 0. This is also reasonable, indicating that most passengers traveled with no or very few parents/children, with some passengers possibly traveling with a larger family.\n",
        "- **SibSp**: The maximum value is 8, and the minimum value is 0. This is still reasonable, as it represents the number of siblings/spouses aboard, and the values are within expected limits for most passengers.\n",
        "- **Fare**: The maximum value is 512.33, and the minimum value is 0. While a few extreme fares might suggest VIP passengers, values up to 512 are plausible in a first-class or luxury context. A cap might be applied to limit extreme values if necessary for modeling purposes.\n",
        "\n",
        "In summary, none of these variables have extreme outliers that would need aggressive treatment, but further action might be needed for **Fare** if there are specific business logic concerns or if the values are heavily skewed.\n"
      ],
      "metadata": {
        "id": "r4nGo73WsFRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Apply log transformation to the 'fare' column\n",
        "df['fare_log'] = np.log1p(df['fare'])  # log1p to handle 0 values\n",
        "\n",
        "# Create subplots to compare original and log-transformed distributions\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Original Fare distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['fare'], kde=True, color='blue')\n",
        "plt.title('Original Fare Distribution')\n",
        "\n",
        "# Log-transformed Fare distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['fare_log'], kde=True, color='green')\n",
        "plt.title('Log Transformed Fare Distribution')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zjg5_g1ls4aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure 'fare' is positive by adding a constant if necessary\n",
        "df['fare_boxcox'] = df['fare'] + 1  # Box-Cox requires positive values, so we add 1 if there are zero values\n",
        "\n",
        "# Apply Box-Cox transformation\n",
        "df['fare_boxcox'], lambda_value = stats.boxcox(df['fare_boxcox'])\n",
        "\n",
        "# Create subplots to compare original and Box-Cox transformed distributions\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Original Fare distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['fare'], kde=True, color='blue')\n",
        "plt.title('Original Fare Distribution')\n",
        "\n",
        "# Box-Cox transformed Fare distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['fare_boxcox'], kde=True, color='red')\n",
        "plt.title('Box-Cox Transformed Fare Distribution')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Output the lambda value\n",
        "print(f\"Lambda value used for Box-Cox transformation: {lambda_value}\")\n"
      ],
      "metadata": {
        "id": "NSj742_MtRyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "kerK6bP0tZmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for imbalance in categorical columns\n",
        "df.select_dtypes(include=['object', 'category']).apply(lambda x: x.value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "kytXYjzzwX9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imbalance Analysis of the 'class' Variable\n",
        "\n",
        "The `class` variable shows the following proportions for each class:\n",
        "\n",
        "| Class | Proportion |\n",
        "|-------|------------|\n",
        "| 3     | 51.66%     |\n",
        "| 1     | 27.30%     |\n",
        "| 2     | 21.05%     |\n",
        "\n",
        "#### Interpretation:\n",
        "- **Imbalance**: The dataset is imbalanced with more passengers from the third class (`class 3`) compared to the other two classes.\n",
        "- **Action**: If you're using this variable for modeling, you may need to address this imbalance, as it could affect model performance. Techniques like **oversampling**, **undersampling**, or using **weighted loss functions** can help mitigate the impact of imbalanced classes.\n"
      ],
      "metadata": {
        "id": "a_NrX2npsJQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "mB54v0LGuiEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning Completed\n",
        "\n",
        "In this phase, we have successfully completed the data cleaning process. The dataset now contains 784 entries with no missing values. All columns are properly cleaned and transformed:\n",
        "\n",
        "- Categorical variables were encoded numerically.\n",
        "- Missing data was imputed using appropriate strategies.\n",
        "- Outliers were handled, including the transformation of the `fare` column using log and Box-Cox transformations to reduce the impact of extreme values.\n",
        "\n",
        "We now have a clean dataset that is ready for further analysis and model building."
      ],
      "metadata": {
        "id": "pmXmx5-Bu7gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DESCRIPTIVE STATISTICS"
      ],
      "metadata": {
        "id": "q2dBKenpw6hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "qpJXQPMMw-Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Insights from Descriptive Statistics:\n",
        "\n",
        "1. **Survived**:  \n",
        "   - The **mean** value of `0.412` indicates that approximately 41.2% of the passengers survived, which aligns with historical data for the Titanic. The **standard deviation** of `0.493` suggests a relatively even split between survivors and non-survivors.\n",
        "   - The **max** of `1` and **min** of `0` confirm that this is a binary variable (survived or not).\n",
        "\n",
        "2. **Pclass**:  \n",
        "   - The mean of `1.244` and **standard deviation** of `0.855` suggests the majority of passengers were from second and third classes, with a smaller proportion in the first class. The **min** and **max** of `0` and `2` suggest that `pclass` is coded from `0` to `2`, but typically, it is considered as `1`, `2`, or `3`.\n",
        "   - The data points toward a **larger proportion** of passengers being from the lower classes, which is consistent with historical demographics.\n",
        "\n",
        "3. **Sex**:  \n",
        "   - With a mean of `0.626`, this suggests that more than half of the passengers were male (coded as `1` for male and `0` for female). The **standard deviation** of `0.484` indicates a relatively balanced distribution between males and females.\n",
        "   - The min of `0` and max of `1` confirms the binary nature of this categorical feature.\n",
        "\n",
        "4. **Age**:  \n",
        "   - The **mean** age of `29.59` years with a **standard deviation** of `13.90` shows a somewhat young demographic. However, the large **min** of `0.42` and **max** of `80` indicate the presence of outliers, such as very young or elderly passengers. This could require **further cleaning** (e.g., capping or imputation) to avoid distorting the model.\n",
        "   - The **median (50%)** is `28`, which supports the idea that the data is somewhat centered around young adults.\n",
        "\n",
        "5. **SibSp (Siblings/Spouses Aboard)** and **Parch (Parents/Children Aboard)**:  \n",
        "   - Both features have a **mean of approximately 0.5**, suggesting that most passengers traveled alone or with only one family member. The **max** values of `8` (for SibSp) and `6` (for Parch) are outliers, indicating that a few passengers traveled with large families. These extreme values might need to be capped or treated as outliers.\n",
        "   - The **standard deviations** of `0.986` and `0.837` confirm the variability, with many passengers traveling alone but a small proportion traveling with large families.\n",
        "\n",
        "6. **Fare**:  \n",
        "   - The mean fare of `34.71` is quite lower than the **max** fare of `512.33`. This high variance (**std = 52.16**) suggests there are a few high-fare passengers, likely those in first class. The large **max** fare indicates potential outliers, which might have a disproportionate impact on model performance. **Log or Box-Cox transformations** could be beneficial to stabilize this skewed distribution.\n",
        "\n",
        "7. **Embarked**:  \n",
        "   - The **mean** of `1.529` and **standard deviation** of `0.803` suggest a fairly balanced distribution of embarked passengers across three ports. Since `1` and `2` represent two major embarkation points, this could indicate that the majority of passengers embarked from those two places, with fewer passengers from the third port.\n",
        "\n",
        "8. **Who**:  \n",
        "   - The feature **Who** seems to have two values, with the mean of `1.529`, which corresponds to a slight imbalance, where more passengers were adults, as expected.\n",
        "\n",
        "9. **Alive** and **Alone**:  \n",
        "   - Both features are binary variables, with **means** around `0.41` for **Alive** (indicating a survival rate of 41%) and `0.57` for **Alone**, showing that more passengers were traveling alone.\n",
        "  \n",
        "10. **Fare_log and Fare_boxcox**:  \n",
        "    - These transformed versions of the `Fare` variable show a more balanced distribution with lower values (suggested by the **mean** and **standard deviation**), indicating that the transformations have reduced the extreme influence of the high-fare passengers.\n",
        "\n",
        "### Key Takeaways:\n",
        "- **Skewness in Fare**: There's a strong right-skew in the `Fare` column, with a few passengers paying significantly higher fares. Transforming this variable could help stabilize variance and reduce the impact of outliers.\n",
        "- **Age Distribution**: The age distribution is centered around younger passengers, but there are some extreme values (ages as low as 0.42 and as high as 80), suggesting that the data may contain errors or outliers.\n",
        "- **Passenger Class**: Most passengers belong to the second and third classes, with fewer in the first class. This aligns with the socioeconomic profile of the Titanic passengers.\n",
        "- **Family Size**: Most passengers traveled alone or with just one family member, but some extreme cases with large family sizes may need to be addressed in further analysis.\n",
        "\n",
        "These insights should guide data preprocessing, feature engineering, and further analysis.\n"
      ],
      "metadata": {
        "id": "_Dv_3uNky6bN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Heatmap"
      ],
      "metadata": {
        "id": "GtnrI7xaz7fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar=True, linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W1sUN2ilz9RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix"
      ],
      "metadata": {
        "id": "I9ZYQmkT0Dtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Interpretation and Insights from the Correlation Matrix\n",
        "\n",
        "## 1. **Survival Rate and Other Variables:**\n",
        "- **Survived vs. Pclass:** A moderate negative correlation of **-0.3327** between `survived` and `pclass` indicates that passengers in lower classes (higher `pclass` values) were less likely to survive.\n",
        "- **Survived vs. Sex:** A strong negative correlation of **-0.5158** suggests that females were more likely to survive than males, which aligns with historical data showing women and children had higher survival rates.\n",
        "- **Survived vs. Age:** The correlation of **-0.0681** is very low, indicating that age had little impact on survival.\n",
        "- **Survived vs. Fare:** A moderate positive correlation of **0.2468** suggests that passengers who paid higher fares were slightly more likely to survive, potentially due to better access to lifeboats or cabins.\n",
        "- **Survived vs. Embarked (Embarkation Location):** The correlation of **-0.1549** suggests a weak negative relationship with survival. This might imply that the place of embarkation had a small influence, but the effect is not strong.\n",
        "\n",
        "## 2. **Class and Survival:**\n",
        "- **Pclass vs. Sex:** A small positive correlation (**0.1153**) suggests that the class might have some relationship with the gender of passengers, but it is not a strong relationship.\n",
        "- **Pclass vs. Fare:** There is a strong negative correlation of **-0.5492**, meaning that passengers in lower classes tended to pay lower fares.\n",
        "- **Pclass vs. Embarked:** A weak positive correlation (**0.1746**) indicates a minor relationship between the class and embarkation point, though it's not a decisive factor.\n",
        "\n",
        "## 3. **Fare and Class:**\n",
        "- **Fare vs. Pclass:** A strong negative correlation of **-0.5492** suggests that higher fares were associated with higher class passengers (first and second class), as expected.\n",
        "- **Fare vs. Sex:** A negative correlation (**-0.1695**) shows that males paid slightly lower fares than females, though the relationship is weak.\n",
        "- **Fare vs. Fare Log / Fare Boxcox:** The high correlation with **fare_log (0.7956)** and **fare_boxcox (0.7505)** suggests that these transformations (logarithmic and Box-Cox transformations) of fare are highly correlated with the original fare value, which is typical when these transformations are used for normality.\n",
        "\n",
        "## 4. **Who and Survival:**\n",
        "- **Who vs. Survived:** A moderate positive correlation (**0.4403**) suggests that being a woman (or child) increased the chances of survival, as historically women and children were prioritized in lifeboat access.\n",
        "- **Who vs. Adult Male:** A strong positive correlation (**0.8990**) suggests that the variable `who` (which likely refers to gender) is strongly associated with `adult_male`.\n",
        "\n",
        "## 5. **Age and Class/Family Relations:**\n",
        "- **Age vs. Sibsp (siblings/spouses aboard):** A moderate negative correlation (**-0.2857**) indicates that older passengers had fewer siblings or spouses aboard. This might suggest that younger passengers tended to travel with family.\n",
        "- **Age vs. Parch (parents/children aboard):** A similar negative correlation (**-0.1870**) suggests older passengers might have fewer children or parents aboard.\n",
        "\n",
        "## 6. **Alone Variable:**\n",
        "- **Alone vs. Sibsp and Parch:** The variable `alone` (indicating whether a passenger was traveling alone) is negatively correlated with both `sibsp (-0.6095)` and `parch (-0.5711)`, suggesting that those with more family members aboard were less likely to be alone.\n",
        "- **Alone vs. Survived:** A negative correlation (**-0.1766**) indicates that passengers traveling alone had a lower chance of survival. This makes sense since passengers with family members might have had more support in emergencies.\n",
        "\n",
        "## 7. **Transformations (Fare_log and Fare_boxcox):**\n",
        "- **Fare_log vs. Fare_boxcox:** The perfect correlation of **0.9961** indicates that these transformations of the `fare` feature are nearly identical in structure, which is typical for common transformations applied to skewed data to make it more normal.\n",
        "- **Fare_log vs. Survived:** A moderate positive correlation (**0.3131**) indicates that passengers with higher transformed fares also had a better chance of survival, supporting the idea that those with higher fares were more likely to have had better access to lifeboats or other advantages.\n",
        "\n",
        "## **Summary of Insights:**\n",
        "1. **Gender and Class are significant predictors of survival:** The strong negative correlations between `sex` and `survived` suggest women were more likely to survive. The negative correlation between `pclass` and `survived` suggests passengers in lower classes had lower survival chances.\n",
        "2. **Fare has a moderate positive relationship with survival:** Passengers who paid higher fares were somewhat more likely to survive, potentially due to better access to lifeboats or cabins.\n",
        "3. **Age has a minor impact on survival:** Age does not significantly affect the likelihood of survival, although there is a slight tendency for younger passengers to travel with family.\n",
        "4. **Traveling Alone vs. Survival:** Those traveling alone were less likely to survive, which might reflect the tendency for passengers with family to have better access to safety resources.\n",
        "5. **Log and Box-Cox Transformed Fare:** The transformations on fare are highly correlated with the original fare, suggesting these are effective for modeling purposes, especially for normalizing skewed data.\n",
        "\n",
        "These insights can be further explored in predictive modeling tasks to refine understanding and improve predictive accuracy, especially for survival prediction based on these variables.\n"
      ],
      "metadata": {
        "id": "3MM7IFB81PAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection\n",
        "\n",
        "Feature Selection is a crucial step to improve your model’s performance by eliminating irrelevant or redundant features. Feature selection can help to:\n",
        "\n",
        "- **Reduce overfitting**\n",
        "- **Improve model accuracy**\n",
        "- **Reduce training time**\n",
        "- **Simplify the model**\n",
        "\n",
        "## 1. Univariate Feature Selection\n",
        "\n",
        "We can start by selecting the most relevant features using statistical tests.\n",
        "\n",
        "For this, we’ll use **SelectKBest** with a statistical test like the **Chi-Squared test** for categorical features and **ANOVA F-test** for continuous features.\n",
        "\n",
        "## 2. Correlation-based Feature Selection\n",
        "\n",
        "We can also remove highly correlated features to avoid multicollinearity, which can skew model performance. This is done by calculating the correlation matrix and identifying features that are highly correlated (e.g., correlation > 0.85 or < -0.85). Then remove one of the features in such pairs to improve model performance.\n",
        "\n",
        "## 3. Recursive Feature Elimination (RFE)\n",
        "\n",
        "RFE is an efficient method to recursively eliminate features and build models, selecting the most important features. This can be done using a model like Logistic Regression, and by iteratively removing the least significant features based on model performance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2H9ivUg31QIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "yRhopeKL4Knl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Q5ASDN_k4Qce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wQAE5xiF2jLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sns\n",
        "\n",
        "# # Features and target variable\n",
        "X = df.drop(columns=['survived'])  # Features\n",
        "y = df['survived']  # Target\n",
        "\n",
        "# # Step 1: Train Random Forest Classifier to get feature importance\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# # Step 2: Get feature importances\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "})\n",
        "\n",
        "# # Step 3: Sort the features by importance (descending order)\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# # Step 4: Normalize the importance to percentage\n",
        "feature_importance['Importance (%)'] = feature_importance['Importance'] * 100\n",
        "\n",
        "# # Step 5: Visualize the feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance (%)', y='Feature', data=feature_importance, palette='Blues_d')\n",
        "\n",
        "# # Annotate with percentage values\n",
        "for index, value in enumerate(feature_importance['Importance (%)']):\n",
        "    plt.text(value + 1, index, f'{value:.2f}%', va='center', fontweight='bold')\n",
        "\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.xlabel('Importance (%)')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Print the feature importance in descending order\n",
        "print(\"Feature Importance in Descending Order (as percentage):\")\n",
        "print(feature_importance[['Feature', 'Importance (%)']])\n"
      ],
      "metadata": {
        "id": "ESZwiwZu8LYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Data preparation\n",
        "X = df.drop(columns='survived')  # Features\n",
        "y = df['survived']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "y_pred = rf.predict(X_test_scaled)\n",
        "\n",
        "# Get classification report\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Convert the report to a DataFrame for better readability\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df"
      ],
      "metadata": {
        "id": "HaFYJVWU_z-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "metadata": {
        "id": "c2bn_zxYDBrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7dDJcyitCwp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve\n",
        "y_pred_prob = rf.predict_proba(X_test_scaled)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EWCU-nNiC39v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment**\n",
        "\n",
        "## **Assignment 1: Feature Importance & Multicollinearity**\n",
        "Using the cleaned data under `df`, you are expected to:\n",
        "\n",
        "1. Implement a feature importance method different from the ones already experimented with in this case.\n",
        "2. Check for multicollinearity and covariance in the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## **Assignment 2: Data Distributions & Visualization**\n",
        "1. Work with at least five different types of data distributions.\n",
        "2. Visualize them using synthetic data.\n"
      ],
      "metadata": {
        "id": "Bm25Tw566vqG"
      }
    }
  ]
}